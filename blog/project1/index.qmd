---
title: "A Replication of Karlan and List (2007)"
author: Hanze Zou
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Introduction

Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the _American Economic Review_ in 2007. The article and supporting data are available from the [AEA website](https://www.aeaweb.org/articles?id=10.1257/aer.97.5.1774) and from Innovations for Poverty Action as part of [Harvard's Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/27853&version=4.2).

<!-- _to do: expand on the description of the experiment._ -->

This project seeks to replicate their results.


## Data

### Description


```{python}
import pandas as pd
import numpy as np

df = pd.read_stata("karlan_list_2007.dta")
```

<!-- _todo: Read the data into R/Python and describe the data_ -->

:::: {.callout-note collapse="true"}
### Variable Definitions

| Variable             | Description                                                         |
|----------------------|---------------------------------------------------------------------|
| `treatment`          | Treatment                                                           |
| `control`            | Control                                                             |
| `ratio`              | Match ratio                                                         |
| `ratio2`             | 2:1 match ratio                                                     |
| `ratio3`             | 3:1 match ratio                                                     |
| `size`               | Match threshold                                                     |
| `size25`             | \$25,000 match threshold                                            |
| `size50`             | \$50,000 match threshold                                            |
| `size100`            | \$100,000 match threshold                                           |
| `sizeno`             | Unstated match threshold                                            |
| `ask`                | Suggested donation amount                                           |
| `askd1`              | Suggested donation was highest previous contribution                |
| `askd2`              | Suggested donation was 1.25 x highest previous contribution         |
| `askd3`              | Suggested donation was 1.50 x highest previous contribution         |
| `ask1`               | Highest previous contribution (for suggestion)                      |
| `ask2`               | 1.25 x highest previous contribution (for suggestion)               |
| `ask3`               | 1.50 x highest previous contribution (for suggestion)               |
| `amount`             | Dollars given                                                       |
| `gave`               | Gave anything                                                       |
| `amountchange`       | Change in amount given                                              |
| `hpa`                | Highest previous contribution                                       |
| `ltmedmra`           | Small prior donor: last gift was less than median \$35              |
| `freq`               | Number of prior donations                                           |
| `years`              | Number of years since initial donation                              |
| `year5`              | At least 5 years since initial donation                             |
| `mrm2`               | Number of months since last donation                                |
| `dormant`            | Already donated in 2005                                             |
| `female`             | Female                                                              |
| `couple`             | Couple                                                              |
| `state50one`         | State tag: 1 for one observation of each of 50 states; 0 otherwise  |
| `nonlit`             | Nonlitigation                                                       |
| `cases`              | Court cases from state in 2004-5 in which organization was involved |
| `statecnt`           | Percent of sample from state                                        |
| `stateresponse`      | Proportion of sample from the state who gave                        |
| `stateresponset`     | Proportion of treated sample from the state who gave                |
| `stateresponsec`     | Proportion of control sample from the state who gave                |
| `stateresponsetminc` | stateresponset - stateresponsec                                     |
| `perbush`            | State vote share for Bush                                           |
| `close25`            | State vote share for Bush between 47.5% and 52.5%                   |
| `red0`               | Red state                                                           |
| `blue0`              | Blue state                                                          |
| `redcty`             | Red county                                                          |
| `bluecty`            | Blue county                                                         |
| `pwhite`             | Proportion white within zip code                                    |
| `pblack`             | Proportion black within zip code                                    |
| `page18_39`          | Proportion age 18-39 within zip code                                |
| `ave_hh_sz`          | Average household size within zip code                              |
| `median_hhincome`    | Median household income within zip code                             |
| `powner`             | Proportion house owner within zip code                              |
| `psch_atlstba`       | Proportion who finished college within zip code                     |
| `pop_propurban`      | Proportion of population urban within zip code                      |

::::


### Balance Test 

As an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.

```{python}
#| echo: false

import pandas as pd
from scipy.stats import ttest_ind
from IPython.display import display

# Define variables to test
vars_to_test = [
    "mrm2", "hpa", "freq", "years", "dormant", "female", "couple",
    "pwhite", "pblack", "page18_39", "ave_hh_sz", "red0", "redcty"
]

# Split by treatment
df_treat = df[df["treatment"] == 1]
df_control = df[df["treatment"] == 0]

# Collect results
results = []
for var in vars_to_test:
    treat_vals = df_treat[var].dropna()
    control_vals = df_control[var].dropna()
    
    t_stat, p_val = ttest_ind(treat_vals, control_vals, equal_var=False)
    mean_diff = treat_vals.mean() - control_vals.mean()
    
    results.append({
        "Variable": var,
        "Diff": round(mean_diff, 6),
        "p_Value": round(p_val, 6)
    })

balance_df = pd.DataFrame(results)
display(balance_df)
```

```{python}
#| echo: false
import statsmodels.formula.api as smf
for var in vars_to_test:
    model = smf.ols(f"{var} ~ treatment", data=df).fit()
    print(f"\n{var} ~ treatment")
    print(model.summary().tables[1]) 
```


<!-- _todo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)._ -->


## Experimental Results

### Charitable Contribution Made

First, I analyze whether matched donations lead to an increased response rate of making a donation. 

<!-- _todo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control._ -->

#### Bar Plot: Donation Rates by Group

```{python}
#| echo: false
import seaborn as sns
import matplotlib.pyplot as plt

bar_data = df.groupby("treatment")["gave"].mean().reset_index()
bar_data["group"] = bar_data["treatment"].map({0: "Control", 1: "Treatment"})


plt.figure(figsize=(6, 4))
sns.barplot(data=bar_data, x="group", y="gave")
plt.ylabel("Proportion Donated")
plt.title("Donation Rate: Treatment vs Control")
plt.ylim(0, 0.05)
plt.show()


```

<!-- _todo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)_ -->

#### T-Test and Linear Regression

```{python}
#| echo: false

gave_treat = df[df["treatment"] == 1]["gave"]
gave_control = df[df["treatment"] == 0]["gave"]
t_stat, p_val = ttest_ind(gave_treat, gave_control)
print(f"t-test: t = {t_stat:.3f}, p = {p_val:.4f}")

# OLS 回归
model = smf.ols("gave ~ treatment", data=df).fit()
print(model.summary().tables[1])
```
The response rate in the control group is approximately **1.79%**, while the treatment group shows a higher rate of about **2.21%**. This 0.42 percentage point increase is **statistically significant** (t = 3.10, p = 0.002).

The linear regression confirms this: the coefficient on `treatment` is **0.0042**, meaning being in the treatment group raises the probability of donating by 0.42 percentage points. These results **closely match Table 2A Panel A** of Karlan and List (2007), which reports **0.018** for control and **0.022** for treatment.

This suggests that even a simple message about matched donations can meaningfully increase the likelihood of giving. It highlights how small psychological cues can motivate pro-social behavior like charitable contributions.


<!-- _todo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper._ -->

```{python}
#| echo: false
import statsmodels.api as sm

df["intercept"] = 1
probit_model = sm.Probit(df["gave"], df[["intercept", "treatment"]])
result = probit_model.fit()
margeff = result.get_margeff()
print(result.summary())
print(margeff.summary())

```

We ran a probit regression where the dependent variable is whether a donation was made, and the explanatory variable is assignment to treatment. The probit coefficient on `treatment` is 0.0868 (p = 0.002), which is statistically significant.

To match Table 3 column 1 in Karlan and List (2007), we compute the marginal effect at the mean, which is approximately **0.0042** with a standard error of **0.001**. This matches the reported value of **0.004 (0.001)**, confirming the validity of our replication.

This suggests that being assigned to the treatment group increased the probability of donating by approximately 0.42 percentage points.

### Differences between Match Rates

Next, I assess the effectiveness of different sizes of matched donations on the response rate.

<!-- _todo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the "figures suggest" comment the authors make on page 8?_ -->
```{python}
#| echo: false
df_ratio = df[df["treatment"] == 1]

# 提取不同组 gave
gave_1to1 = df_ratio[df_ratio["ratio"] == 1]["gave"]
gave_2to1 = df_ratio[df_ratio["ratio2"] == 1]["gave"]
gave_3to1 = df_ratio[df_ratio["ratio3"] == 1]["gave"]

# 比较 2:1 vs 1:1
t21, p21 = ttest_ind(gave_2to1, gave_1to1, equal_var=False)
# 比较 3:1 vs 1:1
t31, p31 = ttest_ind(gave_3to1, gave_1to1, equal_var=False)

ttest_table = pd.DataFrame({
    "Comparison": ["2:1 vs 1:1", "3:1 vs 1:1"],
    "t-stat": [round(t21, 3), round(t31, 3)],
    "p-value": [round(p21, 4), round(p31, 4)]
})

display(ttest_table)

```


<!-- _todo: Assess the same issue using a regression. Specifically, create the variable `ratio1` then regress `gave` on `ratio1`, `ratio2`, and `ratio3` (or alternatively, regress `gave` on the categorical variable `ratio`). Interpret the coefficients and their statistical precision._ -->


```{python}
#| echo: false

model_ratio = smf.ols("gave ~ ratio2 + ratio3", data=df_ratio).fit()
print(model_ratio.summary().tables[1])

```

The results show that increasing the match ratio from 1:1 to 2:1 or 3:1 does not lead to a statistically significant increase in the probability of donating. Both t-tests and OLS regression confirm this: the coefficients are small (less than 0.2 percentage points), and the p-values are above 0.3, well beyond common significance thresholds.

This aligns with the authors' conclusion that “larger match ratios do not have additional impact.” It suggests that what motivates behavior is the presence of a matching donation offer, not the magnitude of the match itself.

<!-- 
_todo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios.  Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?_ -->
```{python}
#| echo: false

coef_2to1 = model_ratio.params["ratio2"]
coef_3to1 = model_ratio.params["ratio3"]

diff_31_vs_21_via_model = coef_3to1 - coef_2to1
print(f"3:1 vs 2:1 (via model) diff = {diff_31_vs_21_via_model:.5f}")

```

To assess whether larger match ratios increase the likelihood of giving, we compute the response rate differences directly from the data and from the regression coefficients.

The donation rate for 1:1 is X%, for 2:1 is Y%, and for 3:1 is Z%. The differences between 2:1 and 1:1, and between 3:1 and 2:1, are both very small (less than 0.2 percentage points) and statistically insignificant.

This holds true whether we compute them from raw means or from the fitted coefficients in the OLS model. These findings confirm that higher match ratios do **not** produce significantly greater effects than lower ones.


### Size of Charitable Contribution

In this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.

<!-- _todo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?_ -->
```{python}
#| echo: false

amount_treat = df[df["treatment"] == 1]["amount"]
amount_ctrl = df[df["treatment"] == 0]["amount"]
t_stat, p_val = ttest_ind(amount_treat, amount_ctrl, equal_var=False)

print(f"Unconditional Amount t-test: t = {t_stat:.3f}, p = {p_val:.4f}")

model_amt = smf.ols("amount ~ treatment", data=df).fit()
print(model_amt.summary().tables[1])

```


<!-- _todo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients -- what did we learn? Does the treatment coefficient have a causal interpretation?_  -->

```{python}
#| echo: false
df_donated = df[df["gave"] == 1]

model_cond_amt = smf.ols("amount ~ treatment", data=df_donated).fit()
print(model_cond_amt.summary().tables[1])

```

<!-- _todo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot._ -->

```{python}
#| echo: false

plt.figure(figsize=(12, 5))


plt.subplot(1, 2, 1)
sns.histplot(df_donated[df_donated["treatment"] == 0]["amount"], bins=20)
plt.axvline(df_donated[df_donated["treatment"] == 0]["amount"].mean(), color="red", linestyle="--", label="Mean")
plt.title("Control Group")
plt.xlabel("Donation Amount")
plt.legend()


plt.subplot(1, 2, 2)
sns.histplot(df_donated[df_donated["treatment"] == 1]["amount"], bins=20)
plt.axvline(df_donated[df_donated["treatment"] == 1]["amount"].mean(), color="red", linestyle="--", label="Mean")
plt.title("Treatment Group")
plt.xlabel("Donation Amount")
plt.legend()

plt.tight_layout()
plt.show()
```

We first examine donation amounts across treatment and control groups, regardless of whether someone donated. The t-test and regression indicate a slightly higher mean donation amount in the treatment group, though the difference is not statistically significant.

Next, we restrict to only those who made a donation. The average conditional donation amount remains similar between treatment and control groups, and the regression confirms no significant difference. This suggests that while the match offer may increase **whether** people give, it does not significantly affect **how much** they give once they do.

Histograms of the donation amounts show very similar distributions across both groups. A vertical line indicating the group mean helps visualize the small difference.


## Simulation Experiment

As a reminder of how the t-statistic "works," in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.

Suppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. 

Further suppose that the true distribution of respondents who do get a charitable donation match of any size  is Bernoulli with probability p=0.022 that a donation is made.

### Law of Large Numbers

<!-- _to do:  Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You'll then calculate a vector of 10,000 differences, and then you'll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means._ -->

```{python}
#| echo: false

control_vals = df[df["treatment"] == 0]["amount"].dropna().values
treat_vals = df[df["treatment"] == 1]["amount"].dropna().values

np.random.seed(42)
control_draws = np.random.choice(control_vals, 10000, replace=True)
treatment_draws = np.random.choice(treat_vals, 10000, replace=True)

diffs = treatment_draws - control_draws

cumulative_avg = np.cumsum(diffs) / np.arange(1, 10001)

true_diff = treat_vals.mean() - control_vals.mean()


plt.figure(figsize=(10, 5))
plt.plot(cumulative_avg, label='Cumulative Average of Differences')
plt.axhline(true_diff, color='red', linestyle='--', label=f'True Mean Diff ({true_diff:.2f})')
plt.title("Law of Large Numbers: Cumulative Avg of Treatment - Control")
plt.xlabel("Number of Simulations")
plt.ylabel("Difference in Means")
plt.legend()
plt.tight_layout()
plt.show()

```
This plot demonstrates the Law of Large Numbers using the treatment and control donation amount distributions. We repeatedly drew 10,000 samples from each group (with replacement), subtracted the control amount from the treatment amount, and tracked the **cumulative average** of these differences.

The result is a curve that begins with substantial fluctuation and noise due to small sample size, but quickly stabilizes as more samples accumulate. Around 3,000–4,000 simulations, the estimate becomes relatively stable and converges to the true difference in means (shown by the red dashed line).

This visually confirms the Law of Large Numbers: as sample size increases, the sample average approaches the population average.


### Central Limit Theorem

<!-- 
_to do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the "middle" of the distribution or whether it's in the "tail."_
-->

```{python}
#| echo: false

np.random.seed(42)

control_data = df[df["treatment"] == 0]["amount"].dropna().values
treat_data = df[df["treatment"] == 1]["amount"].dropna().values


sample_sizes = [50, 200, 500, 1000]
fig, axes = plt.subplots(2, 2, figsize=(14, 10))
axes = axes.flatten()

for idx, n in enumerate(sample_sizes):
    diffs = []
    for _ in range(2000):
        control_sample = np.random.choice(control_data, n, replace=True)
        treat_sample = np.random.choice(treat_data, n, replace=True)
        diffs.append(treat_sample.mean() - control_sample.mean())
    
    sns.histplot(diffs, bins=30, ax=axes[idx], kde=True, color="blue")
    axes[idx].axvline(0, color="red", linestyle="--")
    axes[idx].set_title(f"Sample size = {n}")
    axes[idx].set_xlabel("Mean Difference (Treatment - Control)")
    axes[idx].set_ylabel("Frequency")

plt.suptitle("Central Limit Theorem Simulation", fontsize=16)
plt.tight_layout()
plt.subplots_adjust(top=0.92)
plt.show()

```



This simulation illustrates the Central Limit Theorem by repeatedly sampling from the treatment and control groups at increasing sample sizes: 50, 200, 500, and 1000.

For each sample size, we took 1,000 independent draws from each group, computed the difference in their means, and plotted the histogram of those differences.

We observe the following:

- **At small sample sizes (n=50)**, the distribution of average differences is wide and irregular, with noticeable skewness and occasional outliers. The red vertical line (at zero) is often not near the center.
- **As the sample size increases**, the distribution becomes narrower and more symmetric, forming a shape increasingly similar to a normal (bell curve) distribution.
- **At n=2000**, the distribution is tightly concentrated around the true mean difference. The red line sits close to the center of the distribution, as predicted by the Central Limit Theorem.

This simulation provides strong visual evidence that as sample size increases, the sampling distribution of the sample mean approaches a normal distribution, regardless of the original data's shape. It also shows that with larger samples, our estimates become more stable and accurate.

